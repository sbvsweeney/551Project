---
title: "STATS 551 Project Report"
author: 'Team 26: Suzy McTaggart, Avery Winn, Melody Wu'
date: "Due April 19 2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(rstanarm)
library(bayestestR)
library(bayesplot)
library(insight)
```

## Research Questions

The overarching research question is—What do these popular songs have in common and why do people like these songs? We can narrow down to the following sub-questions:

1) How do songs’ characteristics correlate with their popularity (with year as a potential random effect)?
2) What are some of the most prevalent characteristics of the top 10 most popular songs in each year? In other words, what are some characteristics that can help predict whether a song will be a hit?
3) What are the typical duration and beats per minute of the popular songs and how do these patterns change over time?


#### Data

```{r data}
#The data was collected by Leonardo Henrique through the website “Organize Your Music” (Link). The documentation on the variables and how they are defined can be found on the Organize Your Music site. The list of 603 songs in the data set are the top songs of the years from 2010 to 2019, according to Billboard.

read_csv("https://raw.githubusercontent.com/sbvsweeney/551Project/main/top10s.csv") %>%
    select(ID, year, bpm, nrgy, dnce, dB, live, val, dur, acous, spch, pop) -> data

head(data)

train <- data[ which(data$year < 2019),]
test <- data[which(data$year >2018),]

head(train)
```

#### Exploratory Data Analysis
```{r plots}
# Beats per Minute Scatterplot and Histogram
plot(train$bpm)
hist(train$bpm)

# Energy Scatterplot and Histogram
plot(train$nrgy)
hist(train$nrgy)

# Danceability Scatterplot and Histogram
plot(train$dnce)
hist(train$dnce)

# Loudness Scatterplot and Histogram
plot(train$dB)
hist(train$dB)

# Liveness Scatterplot and Histogram (untransformed and log transformed)
loglive<-log(train$live)

plot(train$live)
hist(train$live)

hist(loglive)

# Valence Scatterplot and Histogram
plot(train$val)
hist(train$val)

# Duration Scatterplot and Histogram
plot(train$dur)
hist(train$dur)


# Acousticness Scatterplot and Histogram (untransformed and log transformed)
logacous<-log(train$acous)

plot(train$acous)
hist(train$acous)

hist(logacous)


# Speechiness Scatterplot and Histogram (untransformed and log transformed)
logspch<-log(train$spch)

plot(train$spch)
hist(train$spch)

hist(logspch)

# Popularity Scatterplot and Histogram (untransformed and squared transformed)
plot(train$pop)
hist(train$pop)

sqpop<-(train$pop)^2
hist(sqpop)

```

### Analysis/Results

#### Research Question 1: How do songs’ characteristics correlate with their popularity? [Bayesian Linear Regression]

```{r blr-notranform}
# Training Data
lrt <- train[,c("bpm", "nrgy", "dnce", "dB", "live", "val", "dur", "acous", "spch", "pop")]

# Square Transform on the outcome variable in the training set (popularity):
sqtrp<-(lrt$pop)^2
lrt$popsq <- sqtrp

# Test Data
lrtest <- test[,c("bpm", "nrgy", "dnce", "dB", "live", "val", "dur", "acous", "spch", "pop")]

# Square Transform on the outcome variable in the testing set (popularity):
sqtp<-(lrtest$pop)^2
lrtest$popsq <- sqtp
dim(lrtest)
testn<-nrow(lrtest)

# Fitting bayesian linear regression via stan.glm, using gaussian family with identity link to run as linear model -- NO TRANSFORMATION ON THE OUTCOME VARIABLE, 4 Markov chains with 2000 iterations
blrnt<- stan_glm(pop~bpm + nrgy + dnce + dB + live + val + dur + acous + spch , family = gaussian(link = "identity"), data=lrt, algorithm="sampling",seed=551)
blrnt

attributes(blrnt)

# Model Coefficients and Summary
blrnt$coefficients
summary(blrnt, prob=c(.025, .5, .975))

# plotting training outcome to fitted values
plot(lrt$pop, blrnt$fitted.values, ylim=c(55,75))

# prior distribution used for model
prior_summary(blrnt)

# generating test fitted values via model (of note: predict function did not work as expected with this bayesian glm model and predict.glm generated an error)
testfitnt<-blrnt$coefficients[1] + (blrnt$coefficients[2]*lrtest$bpm) + (blrnt$coefficients[3]*lrtest$nrgy) + (blrnt$coefficients[4]*lrtest$dnce) + (blrnt$coefficients[5]*lrtest$dB) + (blrnt$coefficients[6]*lrtest$live) + (blrnt$coefficients[7]*lrtest$val) + (blrnt$coefficients[8]*lrtest$dur) + (blrnt$coefficients[9]*lrtest$acous) + (blrnt$coefficients[10]*lrtest$spch)

testfitnt

# trying predict.glm function instead
#predict.glm(blrnt,lrtest)

# sanity check - same number of predictions as observations in the test set
length(testfitnt)
length(lrtest$pop)

# plotting test set outcome to fitted values
plot(lrtest$pop,testfitnt)

# mean square error
testresidnt<-lrtest$pop-testfitnt
resqnt<-sum(testresidnt^2)
msent<-resqnt/31
msent
```

```{r blr-notranform2}
# Fitting bayesian linear regression via stan.glm, using gaussian family with identity link to run as linear model -- NO TRANSFORMATION ON THE OUTCOME VARIABLE, 4 Markov chains with 2000 iterations
blrnt2<- stan_glm(pop~ nrgy + dB, family = gaussian(link = "identity"), data=lrt, algorithm="sampling",seed=551)
blrnt2


# Model Coefficients and Summary
blrnt2$coefficients
summary(blrnt2, prob=c(.025, .5, .975))

# plotting training outcome to fitted values
plot(lrt$pop, blrnt2$fitted.values, ylim=c(55,75))

# prior distribution used for model
prior_summary(blrnt2)

# generating test fitted values via model (of note: predict function did not work as expected with this bayesian glm model and predict.glm generated an error)
testfitnt2<-blrnt2$coefficients[1] + (blrnt2$coefficients[2]*lrtest$nrgy) + (blrnt2$coefficients[3]*lrtest$dB)

testfitnt2

# trying predict.glm function instead
#predict.glm(blrnt,lrtest)

# sanity check - same number of predictions as observations in the test set
length(testfitnt2)
length(lrtest$pop)

# plotting test set outcome to fitted values
plot(lrtest$pop,testfitnt2)

# mean square error
testresidnt2<-lrtest$pop-testfitnt2
resqnt2<-sum(testresidnt2^2)
msent2<-resqnt2/31
msent2
```

```{r blr-sqtransform, message=FALSE}
# Fitting bayesian linear regression via stan.glm, using gaussian family with identity link to run as linear model -- SQUARE TRANSFORMATION ON THE OUTCOME VARIABLE, 4 Markov chains with 2000 iterations
blr<- stan_glm(popsq~bpm + nrgy + dnce + dB + live + val + dur + acous + spch , family = gaussian(link = "identity"), data=lrt, algorithm="sampling",seed=551)
blr

# Model Coefficients and Summary
blr$coefficients
summary(blr, prob=c(.025, .5, .975))

# plotting training outcome to fitted values
plot(lrt$popsq, blr$fitted.values)

# prior distribution used for model
prior_summary(blr)

# generating test fitted values via model 
testfit<-blr$coefficients[1] + (blr$coefficients[2]*lrtest$bpm) + (blr$coefficients[3]*lrtest$nrgy) + (blr$coefficients[4]*lrtest$dnce) + (blr$coefficients[5]*lrtest$dB) + (blr$coefficients[6]*lrtest$live) + (blr$coefficients[7]*lrtest$val) + (blr$coefficients[8]*lrtest$dur) + (blr$coefficients[9]*lrtest$acous) + (blr$coefficients[10]*lrtest$spch)

testfit

# sanity check - same number of predictions as observations in the test set
length(testfit)
length(lrtest$popsq)

# plotting test set outcome to fitted values
plot(lrtest$popsq,testfit)

# mean square error
testresid<-sqrt(lrtest$popsq)-sqrt(testfit)
resq<-sum(testresid^2)
mse<-resq/31
mse
```

```{r blr-sqtransform-fitting1}
blr2<- stan_glm(popsq~nrgy + dnce + dB + live, family = gaussian(),data=lrt, algorithm="sampling", seed=551)
blr2

# Model Coefficients and Summary
summary(blr2, prob=c(.025, .5, .975))

# plotting training outcome to fitted values
plot(lrt$popsq, blr2$fitted.values)

# generating test fitted values via model 
testfit2<-blr2$coefficients[1] + (blr2$coefficients[2]*lrtest$nrgy) + (blr2$coefficients[3]*lrtest$dnce) + (blr2$coefficients[4]*lrtest$dB) + (blr2$coefficients[5]*lrtest$live)

testfit2

# sanity check - same number of predictions as observations in the test set
length(testfit2)
length(lrtest$popsq)

# plotting test set outcome to fitted values
plot(lrtest$popsq,testfit2)

# mean square error
testresid2<-sqrt(lrtest$popsq)-sqrt(testfit2)
resq2<-sum(testresid2^2)
mse2<-resq2/31
mse2

```

```{r blr-sqtransform-fitting2}
blr3<- stan_glm(popsq~nrgy + dB, family = gaussian(),data=lrt, algorithm="sampling", seed=551)
blr3

# Model Coefficients and Summary
summary(blr3, prob=c(.025, .5, .975))

# plotting training outcome to fitted values
plot(lrt$popsq, blr3$fitted.values)

# generating test fitted values via model 
testfit3<-blr3$coefficients[1] + (blr2$coefficients[2]*lrtest$nrgy) + (blr2$coefficients[3]*lrtest$dB)
testfit3

# sanity check - same number of predictions as observations in the test set
length(testfit3)
length(lrtest$popsq)

# plotting test set outcome to fitted values
plot(lrtest$popsq,testfit3)
plot(sqrt(lrtest$popsq),sqrt(testfit3),xlab="Popularity (Test Set)", ylab="Fitted Value")
abline(lm(sqrt(testfit3) ~ sqrt(lrtest$popsq)))

# mean square error
testresid3<-sqrt(lrtest$popsq)-sqrt(testfit3)
resq3<-sum(testresid3^2)
mse3<-resq3/31
mse3

```

```{r blrplots}
# Distribution of Model 3 coefficients

mcmc_dens(blr3, pars = c("nrgy"))+
  vline_at(-15.3, col="red") + vline_at(-24.5, col="blue") + vline_at(-5.9, col="blue")

mcmc_dens(blr3, pars = c("dB"))+
  vline_at(106.5, col="red") + vline_at(52.9, col="blue") + vline_at(160.5, col="blue")

# Distribution of Model 1 coefficients

mcmc_dens(blr, pars = c("bpm"))+
  vline_at(2.0, col="red")

mcmc_dens(blr, pars = c("nrgy"))+
  vline_at(-20.0, col="red")

mcmc_dens(blr, pars = c("dnce"))+
  vline_at(7.6, col="red")

mcmc_dens(blr, pars = c("dB"))+
  vline_at(99.9, col="red")

mcmc_dens(blr, pars = c("live"))+
  vline_at(-6.9, col="red")

mcmc_dens(blr, pars = c("val"))+
  vline_at(1.1, col="red")

mcmc_dens(blr, pars = c("dur"))+
  vline_at(-2.0, col="red")

mcmc_dens(blr, pars = c("acous"))+
  vline_at(-4.8, col="red")

mcmc_dens(blr, pars = c("spch"))+
  vline_at(0.1, col="red")

```

```{r blrpost}
# Description of Posterior Distibution - Model 3
describe_posterior(blr)
#describe_posterior(blr2)

# Description of Posterior Distibution - Model 5 (Final Selected Model)
describe_posterior(blr3)

# Coefficients/Parameters of model posterior distribution - Model 3
post <- get_parameters(blr)
print(purrr::map_dbl(post,median),digits = 3)

# Coefficients/Parameters of model posterior distribution - Model 5 (Final Selected Model)
post <- get_parameters(blr3)
print(purrr::map_dbl(post,median),digits = 3)

# Highest Density Interval - Model 3
hdi(blr)

# Highest Density Interval - Model 5 (Final Selected Model)
hdi(blr3)

```

#### What are some of the most prevalent characteristics of the top 10 most popular songs in each year?

```{r}
library(tidyverse)
```

### Exploratory Data Analysis

```{r}
songs = read.table("top10s.csv", sep = ",", header = TRUE)[,-1]
head(songs)
```

```{r}
ggplot(songs, aes(year)) + geom_bar() + ggtitle("Number of Songs from Each Year")
```

### Loading Data

```{r}
set.seed(551)
songs = read.table("top10s.csv", sep = ",", header = TRUE)[,-1]
songs = songs %>% group_by(year) %>%
  mutate(rank = row_number()) %>% ungroup() %>%
  mutate(top10 = as.factor(ifelse(rank <= 10, TRUE, FALSE))) %>% ungroup()
songs$top10 = as.factor(songs$top10)
colnames(songs) = c("Title", "Artist", "Genre", "Year", "BeatsPerMinute", "Energy", "Danceability",
                    "Loudness", "Liveness", "Valence", "Duration",
                    "Acousticness", "Speachiness", "Popularity", "Rank", "Top10")
```

```{r}
# splitting into training and test sets
train = songs %>% filter(Year != 2019) %>%
  dplyr::select(-c(Title, Artist, Genre, Year, Rank, Popularity))
test = songs %>% filter(Year == 2019) %>%
  dplyr::select(-c(Title, Artist, Genre, Year, Rank, Popularity))
```

```{r}
# creating distribution plots of each variable
filter(train, Top10 == FALSE)[,-10] %>%
  gather(metric, value) %>% 
  ggplot(aes(value, fill = metric)) + 
  geom_density(show.legend = FALSE) + 
  facet_wrap(~ metric, scales = "free")
```

```{r}
# pairwise correlation plot
library(Hmisc)
library(corrplot)
cor = rcorr(as.matrix(train[,-10]), type = "pearson")
corrplot(cor$r, type = "upper", tl.col = "black", tl.srt = 45)
```

### Naive Bayes Classifier

```{r}
library(naivebayes)
# initial NB fit without undersampling failures
m1 = naive_bayes(top10 ~ ., data = train, usekernel = T) 
```

```{r}
train_pred = predict(m1, train, type = "class")
(train_error = table(train_pred, train$top10))
mean(train_pred != train$top10)

test_pred = predict(m1, test, type = "class")
(test_error = table(test_pred, test$top10))
mean(test_pred != test$top10)

test_error / sum(test_error)
```

### Try Undersampling Unpopular Songs

```{r}
# only include top 40 songs for each year
songs = read.table("top10s.csv", sep = ",", header = TRUE)[,-1]
songs = songs %>% group_by(year) %>%
  mutate(rank = row_number()) %>% ungroup() %>%
  mutate(top10 = ifelse(rank <= 10, TRUE, FALSE)) %>%
  filter(rank <= 40)
  
songs$top10 = as.factor(songs$top10)
train = songs %>% filter(year != 2019) %>%
  dplyr::select(-c(title, artist, top.genre, year, rank, pop))
test = songs %>% filter(year == 2019) %>%
  dplyr::select(-c(title, artist, top.genre, year, rank, pop))
```

```{r}
library(naivebayes)
nb1 = naive_bayes(top10 ~ ., data = train, usekernel = T) 
```

```{r}
# training and test errors for naive bayes with raw predictors
train_pred = predict(nb1, train, type = "class")
(train_error = table(train_pred, train$top10))
mean(train_pred != train$top10)

test_pred = predict(nb1, test, type = "class")
(test_error = table(test_pred, test$top10))
mean(test_pred != test$top10)

test_error / sum(test_error)
```

### Logistic Regression

```{r}
train$top10 = ifelse(train$top10 == TRUE, 1, 0)
lr1 = glm(top10 ~ ., data = train, family = "binomial")
train_pred = predict(lr1, train, type = "response")
```


```{r}
# training and test errors for logistic regression with raw predictors
train_pred = predict(lr1, train, type = "response") > 0.5
(train_error = table(train_pred, train$top10))
mean(train_pred != train$top10)

test_pred = predict(lr1, test, type = "response") > 0.5
(test_error = table(test_pred, test$top10))
mean(test_pred != test$top10)

test_error / sum(test_error)
```

### Centering Values by Year

```{r}
# creating dataframe with centered predictors
songs = read.table("top10s.csv", sep = ",", header = TRUE)[,-1] %>% group_by(year) %>%
  mutate(rank = row_number()) %>% ungroup() %>%
  mutate(top10 = as.factor(ifelse(rank <= 10, TRUE, FALSE))) %>% ungroup()
songs$top10 = as.factor(songs$top10)
centered = songs %>% group_by(year) %>%
  mutate(bpm_ct = bpm - mean(bpm),
         nrgy_ct = nrgy - mean(nrgy),
         dnce_ct = dnce - mean(dnce),
         dB_ct = dB - mean(dB),
         live_ct = live - mean(live),
         val_ct = val - mean(val),
         dur_ct = dur - mean(dur),
         acous_ct = acous - mean(acous),
         spch_ct = spch - mean(spch)) %>% ungroup()
centered = centered[,c(4,16:ncol(centered))]
centered$top10 = as.factor(centered$top10)
colnames(centered) = c("Year", "Top10", "BeatsPerMinute", "Energy", "Danceability",
                    "Loudness", "Liveness", "Valence", "Duration",
                    "Acousticness", "Speachiness")
```

```{r}
train = centered %>% filter(Year != 2019)
test = centered %>% filter(Year == 2019)
```

```{r}
library(naivebayes)
nb2 = naive_bayes(top10 ~ ., data = train, usekernel = T) 
```

```{r}
# training and test errors for naive bayes with centering
train_pred = predict(nb2, train, type = "class")
(train_error = table(train_pred, train$top10))
mean(train_pred != train$top10)

test_pred = predict(nb2, test, type = "class")
(test_error = table(test_pred, test$top10))
mean(test_pred != test$top10)

test_error / sum(test_error)
```

```{r}
# centered distributions
train[,-c(1,2)] %>%
  gather(metric, value) %>% 
  ggplot(aes(value, fill = metric)) + 
  geom_density(show.legend = FALSE) + 
  facet_wrap(~ metric, scales = "free")
```

```{r}
train$top10 = ifelse(train$top10 == TRUE, 1, 0)
lr2 = glm(top10 ~ ., data = train, family = "binomial")
train_pred = predict(lr2, train, type = "response")
```

```{r}
# training and test errors for logistic regression with centering
train_pred = predict(lr2, train, type = "response") > 0.5
(train_error = table(train_pred, train$top10))
mean(train_pred != train$top10)

test_pred = predict(lr2, test, type = "response") > 0.5
(test_error = table(test_pred, test$top10))
mean(test_pred != test$top10)

test_error / sum(test_error)
```

```{r}
summary(lr1)
summary(lr2)
```

```{r}
ggplot(train, aes(spch_ct, dur_ct)) + geom_point(aes(color = as.factor(top10)))
```

```{r}
songs %>% group_by(year) %>%
  summarize(mean(dur))
```

```{r}
songs %>% filter(year == 2011, dur > 224, dur < 242)
```

#### What are the typical duration and beats per minute of the popular songs and how do these patterns change over time?


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(tidyverse)
library(glue)
library(aTSA)
library(mvtnorm)
library(greta)
library(bayesplot)
load('EDA-data.Rdata')
```


##############################################################
#### Switch between fast vs. good version of Avery's code ####
```{r}
slow = FALSE
if(!slow) { warning('`slow` is set to FALSE: plots and model summaries will not be reliable') }
```

```{r load-data}
Songs = read.csv('top10s.csv') %>%
  select(-X) %>%
  mutate(year = year - 2009)
```


```{r function-definitions}
plot_beta = function(a, b){
  # plot a beta density (for convenience when choosing prior)
  beta.param = c(a,b)
  d = data.frame(x = seq(0, 1, by=0.01)) %>%
    mutate(y = dbeta(x, beta.param[1], beta.param[2]))
  ggplot(d, aes(x,y)) + geom_line()
}


summary_logis = function(s) {
  # Takes mcmc summary object as argument and applies inverse logit to all values
  s$statistics  = s$statistics %>%
    data.frame %>%
    mutate(SD_below = Mean - SD,
           SD_above = Mean + SD,
           N_SE_below = Mean - Naive.SE,
           N_SE_above = Mean + Naive.SE,
           TS_SE_below = Mean - Time.series.SE,
           TS_SE_above = Mean - Time.series.SE) %>%
    mutate(across(everything(), plogis)) %>%
    mutate(SD = (SD_above-SD_below)/2,
           Naive.SE = (N_SE_above-N_SE_below)/2,
           Time.Series.SE = (TS_SE_below-TS_SE_above)/2)
  s$quantiles = s$quantiles %>% plogis %>% data.frame
  colnames(s$quantiles) = as.character(c('Q.025', 'Q.25', 'Q.5', 'Q.75', 'Q.975'))
  return(s)
}


plot_posterior = function(sim, is.df = FALSE, logit = TRUE,
                          vars = NULL, plot = TRUE) {
  # sim: the output of greta::mcmc
  # is.df: whether sim is a data frame as returned by plot_posterior
  # vars: Which variables should be plotted? Defaults to all
  # plot: plot the posterior, or just return the data frame for plotting?
  
  if(logit==FALSE) {return(NA)}
  
  if(is.df) {sim.df = sim}
  else{ 
    names(sim) = NULL
    sim.df = imap_dfr(sim,
                      ~(.x %>% plogis %>% data.frame %>%
                          rownames_to_column(var='step') %>%
                          mutate(chain=.y, .before=everything()))) %>%
      rename_with(~str_replace(., '\\.$', '')) %>%
      pivot_longer(cols = c(-chain, -step),
                   names_to = 'param', values_to = 'Value') %>%
      separate(param, into=c('Parameter', 'Year', 'metric'), sep='\\.') %>%
      mutate(Year = as.numeric(Year) + 2009,
             metric = factor(metric, labels = metrics3),
             Metric = factor(metric,
                             labels = c('Danceability', 'Energy', 'Valence')))
    if(!is.null(vars)) {
      sim.df %>% filter(Parameter %in% vars)
    }
    if(!plot) {return(sim.df)}
  }
  
  ggplot(sim.df, aes(group=Year, x=Year, y=Value)) +
    geom_violin() +
    scale_x_continuous('Year',
                       breaks = 2010:2019,
                       labels = 2010:2019,
                       sec.axis = dup_axis()) +
    facet_grid(rows=vars(Metric), scales='free_y') +
    labs(title=glue('Posterior distribution for {ifelse(is.null(vars), unique(sim.df$Parameter), vars)}'))
}


write_pdf = function(plot_name) {
  # Write a plot to pdf
  # plot_name: a string specifying the object to be plotted
  file_name = paste0('figures/', plot_name, '.pdf')
  pdf(file_name)
  plot(eval(str2expression(plot_name)))
  dev.off()
}

```

### Heirarchical Bayes

#### Logistic: Sigma_i = Sigma, with mu_i's random

```{r model-definition}
# Set up the hierarchical model with logistic transformation

metrics3 = c('dnce', 'nrgy', 'val')
X.df = Songs %>%
  select(year, metrics3) %>%
  mutate(across(metrics3, ~(./100))) %>%
  mutate(across(metrics3, qlogis)) %>% #LOGISTIC
  filter(if_all(metrics3, is.finite))
X.mat = X.df %>% select(-year) %>% as.matrix
X = X.mat %>% as_data
Xbar = X.df %>% summarize(across(metrics3, mean))
Xbar_i = X.df %>% group_by(year) %>%
  summarize(across(metrics3, mean)) %>% select(-year)

p = ncol(X)
n = nrow(X)

n0 = t(rep(10,p)) # Scales population variance prior estimate
m0 = 6 # Degrees of freedom for sampling Sigma from prior
N0 = 10  # Degrees of freedom for sampling mu from prior
rho0 = 0.2*matrix(1, nrow=p, ncol=p) + 0.8*base::diag(p) # Prior correlation matrix
m_i = 50 # Degrees of freedom for drawing Sigma_i from Sigma
n_i = 10 # Degrees of freedom for drawing mu_i from mu | Sigma_i

mu0_raw = rep(0.6, 3)  # Prior global mean estimate
mu0 = qlogis(mu0_raw)  # (Transformed)

sigma0 = (  # Prior estimate of population variance
  qlogis( mu0_raw + sqrt((1/n0)*mu0_raw*(1-mu0_raw)) )/2 -
  qlogis( mu0_raw - sqrt((1/n0)*mu0_raw*(1-mu0_raw)) )/2)^2
Sigma0 = rho0 * sqrt(t(sigma0)%*%sigma0) # Population covariance prior estimate

# Population covariance and global mean
Sigma = wishart(m0, 1/m0 * Sigma0)
mu = multivariate_normal(t(mu0), 1/N0 * Sigma, dim=3)

# Covariance and mean by year
Sigma_i = Sigma  # Covariance for year i
mu_i = multivariate_normal(mu, 1/n_i * Sigma_i, n_realisations=10)

distribution(X) = multivariate_normal(mu_i[X.df$year,], Sigma_i, n_realisations = n)
```

```{r find-seed}
# The initial value implementation in greta is awful, so it's easiest to just
#   try a bunch of seeds until one doesn't produce NaNs. CANNOT use GPU


good_seed = 1  # I believe 1 is an appropriate seed for the final model
binom3 = model(mu_i)
for(i in 1:ifelse(slow, 500, 50)) {
  tryCatch({
      tensorflow::use_session_with_seed(i, disable_gpu=TRUE,
                                             disable_parallel_cpu=FALSE)
      b_sim3 = mcmc(binom3, n_samples = 1000, warmup=1000,
                    chains=4, one_by_one=TRUE, sampler=rwmh(), thin=1, verbose=FALSE)
      # save(b_sim3, file='models/b_sim3_2.Rdata')
      # write_pdf('b_sim3')
      good_seed = i
      break()
    },
    error=function(e){cat("ERROR :",conditionMessage(e), "\n")}
  )
}
```

```{r best-mcmc}
# Fully implement best model

binom4 = model(mu, mu_i)
tensorflow::use_session_with_seed(1, disable_gpu=TRUE,
                                  disable_parallel_cpu=FALSE)
b_sim4 = mcmc(binom4,
              n_samples = ifelse(slow, 20000, 1000),
              warmup = ifelse(slow, 10000, 1000),
              chains=4, one_by_one=FALSE,
              sampler=rwmh(), thin=2, verbose=TRUE)
b_sim4_sm = summary_logis(summary(b_sim4))
save(b_sim4, file='models/b_sim4.Rdata')
write_pdf('b_sim4')
```

```{r posterior-violin}
# Make faceted violin plot of posterior distributions by year and metric

quant.df = b_sim4_sm$quantiles %>%
  rownames_to_column('param') %>%
  mutate(param = str_replace(param, '\\]', '')) %>%
  separate(param, into=c('Parameter', 'Year'), sep='\\[') %>%
  separate(Year, into=c('Year', 'metric'), sep='\\,') %>%
  filter(Parameter != 'mu' ) %>%
  mutate(Year = as.numeric(Year) + 2009,
         metric = factor(metric, labels = metrics3),
         Metric = factor(metric,
                         labels = c('Danceability', 'Energy', 'Valence')))

p = plot_posterior(b_sim4, vars='mu_i')
(p =  p + labs(title = 'Song metric means by year',
               y = '') +
    geom_linerange(data = quant.df,
                   aes(ymin = Q.025, y = Q.5, ymax = Q.975),
                   size=0.8,
                   color='tomato3'))
if(!slow) { warning('`slow` is set to FALSE: plots and model summaries will not be reliable') }
ggsave('figures/b_sim4_violin_2.pdf', plot = p, height = 5, width = 7)
```

